---
title: Chris Humphrey
layout: default
---

<div class="mid-section-cover" >
    <nav>

        <h1><a href="/" style="color:#000000;text-decoration:none;">{{ site.author_name }}</a></h1>
        

    </nav>
    <main class="sub-mid-section-cover">      
        {{ content }}

<p><strong>The goal is to learn about your customer, not just to lift conversion.</strong></p>
<p><span style="font-weight: 400;">The goal of A/B testing is to learn about your customer. With that mindset, all A/B tests,&nbsp; even the failed ones, present a learning opportunity.&nbsp;</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">After observing a particular facet of customer behavior, ask yourself: what could this behavior reveal about my customer? Maybe customers are making the effort to navigate themselves to a page on the warranty. This could give insight to how important the warranty is to a segment of your customer base.&nbsp;</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">Next, form a hypothesis. The hypothesis should be formatted in this way:</span></p>
<p><span style="font-weight: 400;">If we [do this] then customers may [do this] because [of this reason].</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">Going back to the warranty example, our hypothesis could be the following:</span></p>
<p><span style="font-weight: 400;">If we highlight warranty value on the landing page customers may increase their clickthrough because the warranty is important to them.</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">After running an A/B test on the hypothesis, you may or may not see a change in the conversion rate. Even a &ldquo;failed&rdquo; test is still a success, in that we learned more about our customer. The best A/B testing approach is one where you experiment to learn about your customer, instead of just trying to increase conversion. With this approach, all A/B tests should provide inspiration for future A/B tests.</span></p>
<p>&nbsp;</p>
<p><strong>Design Your A/B Tests in Series</strong></p>
<p><span style="font-weight: 400;">If you approach testing with the mindset to learn about customer psychology, then you should be designing your tests in series. Every test provides a learning opportunity, and that learning opportunity gives insight on what to test next.</span></p>
<p>&nbsp;</p>
<p><strong>Q/A for flickers.</strong></p>
<p><span style="font-weight: 400;">You&rsquo;d be surprised by how many tests I saw companies run that had to be thrown out because of bunk data. Testing platforms are rife with glitches, and the most dangerous A/B tests are those with glitches that go unnoticed.</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">A common and often unnoticed glitch is the &ldquo;flicker.&rdquo; It is more likely to happen on overlay tests. The &ldquo;flicker&rdquo; is where a cookie misfires and counts an individual as being in both the A and B experiences, or neither.</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">You&rsquo;ll want to QA your test on different browsers and devices. After the test is launched bring up the testing platform and website side-by-side. Click around the website and watch the testing platform track you in real time to ensure that data is being collected properly.</span></p>
<p>&nbsp;</p>
<p><strong>Know when to end the test</strong></p>
<p><span style="font-weight: 400;">Many websites do not have enough traffic to A/B test for minor conversion rate lifts at significance. You may never reach conversion, but eventually you&rsquo;ll have to end the test. Instead of looking at the primary KPI, comb through the data to see if there are any other trends in how customer behavior changed. This insight can be just as valuable as the original A/B test.&nbsp;</span></p>
<p>&nbsp;</p>
<p><strong>Target the correct KPI</strong></p>
<p><span style="font-weight: 400;">If a website does not have enough traffic to test final conversion or purchase, then try to find a proxy KPI. This may be clickthrough from the landing page.</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">However, there are dangers in doing this. I once worked with a company that used clickthrough as their proxy KPI, and designed all A/B tests around increasing clickthrough rate. The result was a website where many people clicked through 7 pages, until they came to the final purchase page, and exited the website. The website had a high clickthrough rate, but low conversion rate.&nbsp;</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">Your proxy KPI should be something that correlates well with the conversion rate.</span></p>
<p><br /><br /><br /><br /></p>
<p><span style="font-weight: 400;">&nbsp;</span></p>
        

    </main>
</div>
